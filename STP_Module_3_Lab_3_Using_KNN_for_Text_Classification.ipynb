{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saquibsaqlain272/colab_notebook.IIITH-hub/blob/main/STP_Module_3_Lab_3_Using_KNN_for_Text_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAKrrEmgLZ_J"
      },
      "source": [
        "# **Student Training Program on AIML**\n",
        "### MODULE: CLASSIFICATION-1\n",
        "### LAB-3 : Using KNN for Text Classification\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jeRAsurrdLY"
      },
      "source": [
        "## **Section 1: Understanding NLP tools**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9N2m-HrgWY"
      },
      "source": [
        "In this lab we will be using KNN on a real world NLP application i.e. is text classification. But first look at some NLP techniques for text classification and tools that we use when we want to use python for NLP."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMJhTF89MmT_"
      },
      "source": [
        "## Section 1.2: Data Cleaning and Preprocessing step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRXVjJHk3urf"
      },
      "source": [
        "Raw text must be processed and converted into a form so that it is suitable to use with various machine-learning algorithms.  \n",
        "In case of text, there are lots of things that need to be taken into account.  \n",
        "\n",
        "\n",
        "1.   Removing numbers from the text\n",
        "2.   Handling capitalization and punctuation.\n",
        "3.   Stemming and Lemmatizing text.  \n",
        "\n",
        "And most importantly, one can't just use words or images directly in algorithms; they need to be converted into vectors- a form that algorithms can understand.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYq4np0xqUBr"
      },
      "source": [
        "### **NLTK**\n",
        "NLTK (or Natural Language Tool Kit) is a commonly used library for processing text. We will use this tool in this lab. Lets first install it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wFOZHWV3_ABt",
        "outputId": "d6c0b32f-b606-46ee-9258-06691b686bee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gpgq3SQK5IOr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def cleanText(text, lemmatize, stemmer):\n",
        "    \"\"\"Method for cleaning text from train and test data. Removes numbers, punctuation, and capitalization. Stems or lemmatizes text.\"\"\"\n",
        "\n",
        "    if isinstance(text, float):\n",
        "        text = str(text)\n",
        "    if isinstance(text, numpy.int64):\n",
        "        text = str(text)\n",
        "    try:\n",
        "        text = text.decode()\n",
        "    except AttributeError:\n",
        "        pass\n",
        "\n",
        "    soup = BeautifulSoup(text, \"lxml\")\n",
        "    text = soup.get_text()\n",
        "    text = re.sub(r\"[^A-Za-z]\", \" \", text)\n",
        "    text = text.lower()\n",
        "\n",
        "\n",
        "    if lemmatize:\n",
        "        wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "        def get_tag(tag):\n",
        "            if tag.startswith('J'):\n",
        "                return wordnet.ADJ\n",
        "            elif tag.startswith('V'):\n",
        "                return wordnet.VERB\n",
        "            elif tag.startswith('N'):\n",
        "                return wordnet.NOUN\n",
        "            elif tag.startswith('R'):\n",
        "                return wordnet.ADV\n",
        "            else:\n",
        "                return ''\n",
        "\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)  # Generate list of tokens\n",
        "        tagged = pos_tag(tokens)\n",
        "        for t in tagged:\n",
        "            try:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0], get_tag(t[1][:2])))\n",
        "            except:\n",
        "                text_result.append(wordnet_lemmatizer.lemmatize(t[0]))\n",
        "        return text_result\n",
        "\n",
        "    if stemmer:\n",
        "        text_result = []\n",
        "        tokens = word_tokenize(text)\n",
        "        snowball_stemmer = SnowballStemmer('english')\n",
        "        for t in tokens:\n",
        "            text_result.append(snowball_stemmer.stem(t))\n",
        "        return text_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuBxfzbuzrs1",
        "outputId": "5f3184cd-733e-41c0-c734-143b7488be6e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Troubling\n",
            "troubl\n",
            "trouble\n"
          ]
        }
      ],
      "source": [
        "sample_text = \"Troubling\"\n",
        "sample_text_result = cleanText(sample_text, lemmatize=False, stemmer=True)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text)\n",
        "print(sample_text_result)\n",
        "sample_text_result = cleanText(sample_text, lemmatize=True, stemmer=False)\n",
        "sample_text_result = \" \".join(str(x) for x in sample_text_result)\n",
        "print(sample_text_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcqQi34UoPvq"
      },
      "source": [
        "## Section 1.2: BAG OF WORDS\n",
        "\n",
        "A bag-of-words model, or BoW for short, is a way of extracting features from text for use in modeling, such as with machine learning algorithms.\n",
        "\n",
        "The approach is very simple and flexible, and can be used in many ways for extracting features from documents.\n",
        "\n",
        "A bag-of-words is a representation of text that describes the occurrence of words within a document.\n",
        "It is called a “bag” of words, because any information about the order or structure of words in the document is discarded. The model is only concerned with whether known words occur in the document, not where in the document."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FN8sZqXaNLe7",
        "outputId": "41f0dff3-569b-4eab-c686-a77681d212fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "5*12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YYSpQzIM05l"
      },
      "outputs": [],
      "source": [
        "# Functions to convert document(s) to a list of words, with the option of removing stopwords. Returns document-term matrix.\n",
        "\n",
        "def createBagOfWords(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer = CountVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    bag_of_words_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    bag_of_words_test = vectorizer.transform(clean_test).toarray()\n",
        "    return bag_of_words_train, bag_of_words_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74QgBefsKsZ3",
        "outputId": "0b2a51e3-b624-4b97-a138-5ca87d45ffa9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary (unique words): ['great' 'hate' 'is' 'love' 'movie' 'this']\n",
            "\n",
            "Bag of Words Matrix:\n",
            "[[0 0 0 1 1 1]\n",
            " [1 0 1 0 1 1]\n",
            " [0 1 0 0 1 1]]\n",
            "\n",
            "Each row represents a sentence, each column represents a word count\n"
          ]
        }
      ],
      "source": [
        "# Visualize how Bag of Words works with a simple example\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_sentences = [\n",
        "    \"I love this movie\",\n",
        "    \"This movie is great\",\n",
        "    \"I hate this movie\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "bow_matrix = vectorizer.fit_transform(sample_sentences)\n",
        "\n",
        "print(\"Vocabulary (unique words):\", vectorizer.get_feature_names_out())\n",
        "print(\"\\nBag of Words Matrix:\")\n",
        "print(bow_matrix.toarray())\n",
        "print(\"\\nEach row represents a sentence, each column represents a word count\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v0iCUBqoX82"
      },
      "source": [
        "## Section 1.3: TF-IDF\n",
        "TF-IDF technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.\n",
        "\n",
        "The number of times a term occurs in a document is called its Term frequency (TF).\n",
        "\n",
        " Document frequency is the number of documents in which the word is present.  Inverse DF (IDF) is the inverse of the document frequency which measures the informativeness of term *t*.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6_-DDMQoXEX"
      },
      "outputs": [],
      "source": [
        "def createTFIDF(train, test, remove_stopwords, lemmatize, stemmer):\n",
        "    if remove_stopwords:\n",
        "        vectorizer = TfidfVectorizer(analyzer='word', input='content', stop_words=stopwords.words('english'))\n",
        "    else:\n",
        "        vectorizer =  TfidfVectorizer(analyzer='word', input='content')\n",
        "\n",
        "    clean_train = []\n",
        "    for paragraph in train:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_train.append(paragraph)\n",
        "\n",
        "    clean_test = []\n",
        "    for paragraph in test:\n",
        "        paragraph_result = cleanText(paragraph, lemmatize, stemmer)\n",
        "        paragraph = \" \".join(str(x) for x in paragraph_result)\n",
        "        clean_test.append(paragraph)\n",
        "\n",
        "    tfidf_train = vectorizer.fit_transform(clean_train).toarray()\n",
        "    tfidf_test = vectorizer.transform(clean_test).toarray()\n",
        "    return tfidf_train, tfidf_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zl5qGI9TKxP_",
        "outputId": "ecd66611-c842-45d2-8165-310f884d5571"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.         0.         0.76749457 0.45329466 0.45329466]\n",
            " [0.6088451  0.         0.6088451  0.         0.35959372 0.35959372]\n",
            " [0.         0.76749457 0.         0.         0.45329466 0.45329466]]\n",
            "\n",
            "Notice: Common words like 'movie' have lower scores\n",
            "Unique words like 'love', 'hate', 'great' have higher scores\n"
          ]
        }
      ],
      "source": [
        "# Compare BoW vs TF-IDF on the same example\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "sample_sentences = [\n",
        "    \"I love this movie\",\n",
        "    \"This movie is great\",\n",
        "    \"I hate this movie\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(sample_sentences)\n",
        "\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"\\nNotice: Common words like 'movie' have lower scores\")\n",
        "print(\"Unique words like 'love', 'hate', 'great' have higher scores\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7g0jS45epcC5"
      },
      "source": [
        "# **Section 2: UNDERSTANDING THE DATA : A REVIEWS DATASET**\n",
        "\n",
        "Sentiment analysis is the interpretation and classification of emotions (such as positive, negative and neutral) within text data using text analysis techniques.  \n",
        "Given below is a dataset consisting of reviews along with sentiment class (positive or negative)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "jU6875-NxrHw",
        "outputId": "9a0eb195-fb56-4a5e-dab5-6726b1ce657f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5db84a8f-f318-49e6-99fc-8813e8748eef\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5db84a8f-f318-49e6-99fc-8813e8748eef\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Upload the Reviews CSV file that has been shared with you.\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HILJMpa_y26e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXD0iAR5k62v"
      },
      "outputs": [],
      "source": [
        "df = df.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MTCScT7uOdUy"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j9rNGkQpn7y"
      },
      "outputs": [],
      "source": [
        "df.to_csv('reviews.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdQxSybRKk8O"
      },
      "outputs": [],
      "source": [
        "# Explore the dataset to understand its structure\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nClass Distribution:\")\n",
        "print(df['sentiment'].value_counts())\n",
        "\n",
        "print(\"\\nSample Reviews:\")\n",
        "for i in range(3):\n",
        "    print(f\"\\nReview {i+1}: {df['sentence'].iloc[i]}\")\n",
        "    print(f\"Sentiment: {df['sentiment'].iloc[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QXo11Bxvytu"
      },
      "source": [
        "# **Section 3: KNN MODEL**\n",
        "\n",
        "Given below are two KNN models; in the first case we are using Bag-of-Words and in the second case we are using TF-IDF.\n",
        "Note the different metrics and parameters used in each."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5yZXboZ92Z4"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 1: Tweak the models below and see results with different parameters and distance metrics.\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"], test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2, metric='euclidean', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbor algorithm\"\"\"\n",
        "\n",
        "    training_data = pd.read_csv('reviews.csv')\n",
        "    X_train, X_test, y_train, y_test = train_test_split(training_data[\"sentence\"], training_data[\"sentiment\"],\n",
        "                                                        test_size=0.2, random_state=5)\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True, lemmatize=True, stemmer=False)\n",
        "    # print(X_train)\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance', algorithm='brute', leaf_size=30, p=2,\n",
        "                                         metric='cosine', metric_params=None, n_jobs=1)\n",
        "\n",
        "    knn.fit(X_train, y_train)\n",
        "    predicted = knn.predict(X_test)\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    return predicted, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPuI3wrL-8ZJ"
      },
      "source": [
        "Note: Cross-validation will be discussed in detail in the upcoming lab session."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xzno1rRNHoFT"
      },
      "outputs": [],
      "source": [
        "## KNN accuracy after using BoW\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FI5NMP8L-4eW"
      },
      "outputs": [],
      "source": [
        "## KNN accuracy after using TFIDF\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zWuvayt0K0iK"
      },
      "outputs": [],
      "source": [
        "# Visualize model performance with confusion matrix\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Create confusion matrix\n",
        "cm = confusion_matrix(y_test, predicted)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['Negative', 'Positive'],\n",
        "            yticklabels=['Negative', 'Positive'])\n",
        "plt.title('Confusion Matrix - KNN with TF-IDF')\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, predicted, target_names=['Negative', 'Positive']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vm2vYDVfa5AP"
      },
      "source": [
        "# Section 4: SPAM TEXT DATASET\n",
        "Now let's use what we've learnt to classify texts as spam or not spam."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1GZgvXCsKX1"
      },
      "outputs": [],
      "source": [
        "# Upload the spam text data CSV file that has been shared with you. You can also download the file from https://www.kaggle.com/datasets/uciml/sms-spam-collection-dataset\n",
        "# Run this cell, click on the 'Choose files' button and upload the file.\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRiS7dT7piTE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load spam.csv with encoding fix\n",
        "df = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "# Many spam datasets have extra unnamed columns, so we'll select only the first 2\n",
        "if df.shape[1] > 2:\n",
        "    df = df.iloc[:, :2]  # Keep only first 2 columns\n",
        "\n",
        "# Rename columns to standard names\n",
        "df.columns = ['Category', 'Message']\n",
        "\n",
        "# Remove any missing values\n",
        "df = df.dropna()\n",
        "\n",
        "# Display dataset info\n",
        "print(f\"✓ Dataset loaded successfully!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df['Category'].value_counts())\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsvBm4luNCME"
      },
      "outputs": [],
      "source": [
        "df['Category'] = df['Category'].map({'ham': 0, 'spam': 1})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiEHuMypWyb6"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRJU9rFy1XQR"
      },
      "outputs": [],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wnv0v4T6sqJQ"
      },
      "outputs": [],
      "source": [
        "from sklearn import metrics, neighbors\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "\n",
        "## TASK - 2: Tweak the models below and see results with different parameters\n",
        "\n",
        "def bow_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using bag-of-words and K-Nearest Neighbors.\"\"\"\n",
        "\n",
        "    # Load data with proper encoding\n",
        "    training_data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "    # Clean the data - keep only first 2 columns if there are more\n",
        "    if training_data.shape[1] > 2:\n",
        "        training_data = training_data.iloc[:, :2]\n",
        "    training_data.columns = ['Category', 'Message']\n",
        "    training_data = training_data.dropna()\n",
        "\n",
        "    # Map categories to numeric\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        training_data[\"Message\"],\n",
        "        training_data[\"Category\"],\n",
        "        test_size=0.2,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    # Create Bag of Words features\n",
        "    X_train, X_test = createBagOfWords(X_train, X_test, remove_stopwords=True,\n",
        "                                        lemmatize=True, stemmer=False)\n",
        "\n",
        "    # Train KNN model\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='uniform',\n",
        "                                          metric='euclidean')\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predicted = knn.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with BOW accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    # Cross-validation\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "    print('\\n')\n",
        "\n",
        "    return predicted, y_test\n",
        "\n",
        "\n",
        "def tfidf_knn():\n",
        "    \"\"\"Method for determining nearest neighbors using tf-idf and K-Nearest Neighbors.\"\"\"\n",
        "\n",
        "    # Load data with proper encoding\n",
        "    training_data = pd.read_csv('spam.csv', encoding='latin-1')\n",
        "\n",
        "    # Clean the data - keep only first 2 columns if there are more\n",
        "    if training_data.shape[1] > 2:\n",
        "        training_data = training_data.iloc[:, :2]\n",
        "    training_data.columns = ['Category', 'Message']\n",
        "    training_data = training_data.dropna()\n",
        "\n",
        "    # Map categories to numeric\n",
        "    training_data['Category'] = training_data['Category'].map({'ham': 0, 'spam': 1})\n",
        "\n",
        "    # Split data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        training_data[\"Message\"],\n",
        "        training_data[\"Category\"],\n",
        "        test_size=0.2,\n",
        "        random_state=0\n",
        "    )\n",
        "\n",
        "    # Create TF-IDF features\n",
        "    X_train, X_test = createTFIDF(X_train, X_test, remove_stopwords=True,\n",
        "                                   lemmatize=True, stemmer=False)\n",
        "\n",
        "    # Train KNN model\n",
        "    knn = neighbors.KNeighborsClassifier(n_neighbors=5, weights='distance',\n",
        "                                          metric='cosine', metric_params=None)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    predicted = knn.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    acc = metrics.accuracy_score(y_test, predicted)\n",
        "    print('KNN with TFIDF accuracy = ' + str(acc * 100) + '%')\n",
        "\n",
        "    # Cross-validation\n",
        "    scores = cross_val_score(knn, X_train, y_train, cv=3)\n",
        "    print(\"Cross Validation Accuracy: %0.2f\" % (scores.mean()))\n",
        "    print(scores)\n",
        "\n",
        "    return predicted, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8PwydHYs1h_"
      },
      "outputs": [],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = bow_knn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zf8i2P1nxpl8"
      },
      "outputs": [],
      "source": [
        "# This cell may take some time to run\n",
        "predicted, y_test = tfidf_knn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9xvbzS4yLTr"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "2. Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "3. Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dc8e2ee1"
      },
      "source": [
        "### Questions to Think About and Answer\n",
        "1. Why does the TF-IDF approach generally result in a better accuracy than Bag-of-Words ?\n",
        "2. Can you think of techniques that are better than both BoW and TF-IDF ?\n",
        "3. Read about Stemming and Lemmatization from the resources given below. Think about the pros/cons of each.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a903179"
      },
      "source": [
        "\n",
        "\n",
        "TF-IDF (Term Frequency-Inverse Document Frequency) generally outperforms Bag-of-Words (BoW) for several reasons:\n",
        "\n",
        "1.  **Importance of Words**: BoW simply counts the occurrences of words, treating all words equally. TF-IDF, on the other hand, considers not only how frequently a word appears in a document (Term Frequency - TF) but also how unique or important that word is across the entire corpus (Inverse Document Frequency - IDF). Words that appear frequently in many documents (like 'a', 'the', 'is') are down-weighted by IDF, making more distinguishing words stand out.\n",
        "2.  **Reduces Impact of Common Words**: Common words (stop words) that appear in almost all documents provide little to no discriminative power. While BoW can be combined with stop word removal, TF-IDF inherently reduces the weight of such words, preventing them from dominating the feature vector.\n",
        "3.  **Captures Semantic Information Better (Indirectly)**: By highlighting words that are specific to certain documents or topics, TF-IDF implicitly captures more semantic information than raw word counts. It allows the model to differentiate between documents more effectively based on their unique vocabulary.\n",
        "4.  **Vector Representation**: TF-IDF vectors tend to be more informative and less noisy, leading to better representations for machine learning models, especially in classification tasks where distinguishing between categories is crucial."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cc3a09f"
      },
      "source": [
        "\n",
        "\n",
        "Yes, several techniques offer more advanced and often superior text representation compared to BoW and TF-IDF, especially in terms of capturing semantic meaning and context:\n",
        "\n",
        "1.  **Word Embeddings (e.g., Word2Vec, GloVe, FastText)**:\n",
        "    *   These models learn dense vector representations of words where words with similar meanings are located closer in the vector space. They capture semantic relationships and context, which BoW and TF-IDF completely miss.\n",
        "    *   **Word2Vec**: Creates word embeddings by predicting context from a word (skip-gram) or a word from its context (CBOW).\n",
        "    *   **GloVe**: Global Vectors for Word Representation, an unsupervised learning algorithm for obtaining vector representations for words.\n",
        "    *   **FastText**: An extension of Word2Vec that considers subword information (character n-grams), which helps with out-of-vocabulary words and morphologically rich languages.\n",
        "\n",
        "2.  **Contextual Word Embeddings (e.g., ELMo, BERT, GPT, T5)**:\n",
        "    *   These are more advanced embeddings that generate a word's representation based on its specific context in a sentence. The same word can have different embeddings depending on the words around it.\n",
        "    *   **ELMo (Embeddings from Language Models)**: Generates embeddings based on a deep bidirectional LSTM.\n",
        "    *   **BERT (Bidirectional Encoder Representations from Transformers)**: A powerful pre-trained transformer model that learns deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers.\n",
        "    *   **GPT (Generative Pre-trained Transformer)**: Family of transformer-based models primarily used for language generation tasks, also providing contextual embeddings.\n",
        "\n",
        "3.  **Doc2Vec (Paragraph Vectors)**:\n",
        "    *   An extension of Word2Vec that learns fixed-length feature representations for documents (or paragraphs), similar to how Word2Vec learns representations for words. It can represent entire documents semantically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e27addf7"
      },
      "source": [
        "\n",
        "Both Stemming and Lemmatization are techniques used to reduce words to their base or root form, aiming to improve the accuracy and efficiency of text processing tasks by reducing lexical variation.\n",
        "\n",
        "\n",
        "\n",
        "**Definition**: Stemming is a heuristic process that chops off suffixes from words, often resulting in a base word that is not a dictionary word. It's a faster and simpler approach.\n",
        "\n",
        "**Pros**:\n",
        "*   **Simplicity and Speed**: Stemmers are typically faster to implement and execute compared to lemmatizers because they use algorithmic rules rather than linguistic knowledge.\n",
        "*   **Reduces Vocabulary Size**: Effectively reduces the number of unique tokens, which can help in reducing the dimensionality of feature vectors.\n",
        "\n",
        "**Cons**:\n",
        "*   **Lack of Linguistic Accuracy (Over-stemming/Under-stemming)**:\n",
        "    *   **Over-stemming**: Cuts off too much, making different words identical (e.g., \"universal,\" \"university\" -> \"univers\"). This can lead to loss of meaning.\n",
        "    *   **Under-stemming**: Fails to reduce words that should have been (e.g., \"data,\" \"datum\" might not be stemmed to a common root).\n",
        "*   **Produces Non-Words**: Often results in stems that are not actual words (e.g., \"troubling\" -> \"troubl\"), which can be problematic for tasks requiring human readability or dictionary lookups.\n",
        "*   **Context Ignorance**: Does not consider the context of the word in a sentence.\n",
        "\n",
        "**Example**: `running`, `runs`, `runner` -> `run` (correct); `caring` -> `car` (over-stemming).\n",
        "\n",
        "##### **Lemmatization**\n",
        "\n",
        "**Definition**: Lemmatization is a more sophisticated process that uses vocabulary (dictionary) and morphological analysis of words to return the base or dictionary form of a word, known as a lemma. The lemma is always a valid word.\n",
        "\n",
        "**Pros**:\n",
        "*   **Linguistic Accuracy**: Provides the correct morphological root (lemma) of a word, which is always a valid word and retains meaning.\n",
        "*   **Better for Human Interpretation**: Since it produces real words, the output is more understandable and can be beneficial for tasks where the meaning of the root form is important.\n",
        "*   **Context-Aware (can be)**: Can optionally consider the Part-of-Speech (POS) of a word to determine the correct lemma (e.g., \"meeting\" as a noun versus \"meeting\" as a verb).\n",
        "\n",
        "**Cons**:\n",
        "*   **Computational Cost**: Generally slower and more computationally intensive than stemming because it requires access to a lexicon (dictionary) and morphological analysis.\n",
        "*   **Complexity**: More complex to implement due to the need for linguistic resources.\n",
        "*   **POS Tagging Requirement**: For optimal accuracy, it often requires prior Part-of-Speech tagging, which adds another step to the preprocessing pipeline.\n",
        "\n",
        "**Example**: `running`, `runs`, `ran` -> `run` (correct); `better`, `best` -> `good`.\n",
        "\n",
        "**When to use which**:\n",
        "*   **Stemming** is often preferred when speed is critical, and a slight loss of accuracy or readability in the base form is acceptable (e.g., in information retrieval systems where the goal is just to group similar words for search). Many search engines use stemming.\n",
        "*   **Lemmatization** is generally preferred when higher accuracy, linguistic correctness, and semantic understanding are important (e.g., in natural language understanding, machine translation, or text summarization), even at the cost of increased processing time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6xPL6smyWG-"
      },
      "source": [
        "### Useful Resources for further reading\n",
        "1. Stemming and Lemmatization: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n",
        "2. TF-IDF and BoW : https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/\n",
        "3. TF-IDF: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}